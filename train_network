#!/usr/bin/env python3
from typing import Sequence

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
from torch import Tensor, nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm, trange

EPOCHS = 50
BATCH_SIZE = 128
INPUT_DIMENSION = 4
OUTPUT_DIMENSION = 2

LOSS_FUNCTION = nn.MSELoss
OPTIMIZER = AdamW

# scheduler
LEARNING_RATE = 1e-2
PATIENCE = 5
FACTOR = 1 / 3

NEURONS = [INPUT_DIMENSION, 8, 16, 8, OUTPUT_DIMENSION]


class CustomSubgridDataset(Dataset):
    data: pd.DataFrame
    mean: Sequence[float]
    std: Sequence[float]

    def __init__(self, data_file: str):
        self.data = pd.read_csv(
            data_file, header=[0, 1], skipinitialspace=True, index_col=0
        )
        self.mean = list(self.data.mean())
        self.std = list(self.data.std())

    def __len__(self) -> int:
        return self.data.shape[0]

    def __getitem__(self, index: int):
        coarse_solutions = self.data.iloc[index, :INPUT_DIMENSION]
        subgrid_fluxes = self.data.iloc[index, INPUT_DIMENSION:]

        return torch.tensor(coarse_solutions, dtype=torch.float32), torch.tensor(
            subgrid_fluxes, dtype=torch.float32
        )


class Normalize(nn.Module):
    def __init__(self, mean, std):
        super().__init__()
        self.mean = Tensor(mean)
        self.std = Tensor(std)

    def forward(self, tensor: Tensor) -> Tensor:
        return (tensor - self.mean) / self.std

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}(mean={self.mean}, std={self.std})"


class NeuralNetwork(nn.Sequential):
    def __init__(self, normalize: Normalize):
        super().__init__()
        self.add_module("Normalize", normalize)
        self._build_modules()

    def _build_modules(self):
        # self._add_normalization()

        for layer in range(len(NEURONS) - 1):
            if layer > 0:
                self._add_activation_function(layer)

            self._add_cross(layer)

    def _add_cross(self, layer: int):
        cross_name = f"Layer{layer} -> Layer{layer+1}"
        self.add_module(cross_name, nn.Linear(NEURONS[layer], NEURONS[layer + 1]))

    def _add_dropout(self, layer: int, p: float):
        self.add_module(f"Layer{layer} Dropout", nn.Dropout(p))

    def _add_activation_function(self, layer: int):
        activ_fn_name = f"Layer{layer} Activation Function"
        self.add_module(activ_fn_name, nn.LeakyReLU())


class CustomScheduler(ReduceLROnPlateau):
    def __init__(self, optimizer):
        ReduceLROnPlateau.__init__(
            self, optimizer, verbose=True, patience=PATIENCE, factor=FACTOR
        )

    def _reduce_lr(self, epoch):
        for i, param_group in enumerate(self.optimizer.param_groups):
            old_lr = float(param_group["lr"])
            new_lr = max(old_lr * self.factor, self.min_lrs[i])
            if old_lr - new_lr > self.eps:
                param_group["lr"] = new_lr
                if self.verbose:
                    tqdm.write(
                        "Epoch {:.0f}: reducing learning rate"
                        " of group to {:.4e}.".format(epoch, new_lr)
                    )


def train(dataloader, model, loss_function, optimizer):
    num_batches = len(dataloader)
    model.train()
    training_loss = 0

    for X, y in tqdm(dataloader, leave=False, unit="batch", desc="Train"):
        X, y = X.to(device), y.to(device)

        # Compute prediction error
        pred = model(X)
        loss = loss_function(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        training_loss += loss.item()

    return training_loss / num_batches


def validate(dataloader, model, loss_function):
    num_batches = len(dataloader)
    model.eval()
    validation_loss = 0
    with torch.no_grad():
        for X, y in tqdm(dataloader, leave=False, unit="batch", desc="Validate"):
            X, y = X.to(device), y.to(device)
            pred = model(X)
            validation_loss += loss_function(pred, y).item()
    validation_loss /= num_batches

    return validation_loss


def plot_losses(training_loss, validation_loss):
    epochs = np.arange(1, len(training_loss) + 1)
    plt.plot(epochs, training_loss, label="training loss")
    plt.plot(epochs, validation_loss, label="validation loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.show()


if __name__ == "__main__":
    # Load data
    training_data = CustomSubgridDataset("data/train.csv")
    validation_data = CustomSubgridDataset("data/validate.csv")

    # Create data loaders.
    train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)
    validate_dataloader = DataLoader(
        validation_data, batch_size=BATCH_SIZE, shuffle=True
    )

    # Get cpu or gpu device for training.
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"INFO: Using {device} device.")

    # definie model
    normalize = Normalize(
        training_data.mean[0:INPUT_DIMENSION], training_data.std[0:INPUT_DIMENSION]
    )
    model = NeuralNetwork(normalize).to(device)
    print(model)

    loss_function = LOSS_FUNCTION()
    optimizer = OPTIMIZER(model.parameters(), lr=LEARNING_RATE)
    scheduler = CustomScheduler(optimizer)

    training_loss = np.empty(EPOCHS)
    validate_loss = np.empty(EPOCHS)

    with trange(EPOCHS, desc="Training", unit="epoch", leave=False) as t:
        for epoch in t:
            training_loss[epoch] = train(
                train_dataloader, model, loss_function, optimizer
            )
            validate_loss[epoch] = validate(validate_dataloader, model, loss_function)
            scheduler.step(training_loss[epoch])
            t.set_postfix(loss=training_loss[epoch])

    tqdm.write(f"Reached loss: {min(training_loss)}")
    plot_losses(training_loss, validate_loss)
